{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "## By Varun Krishnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Problem Specification: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "install(\"tensorflow>=1.7.0\")\n",
    "install('tensorflow-hub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class '_frozen_importlib._ModuleLockManager'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spopt\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vnkn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"square_T1.zip\",\"r\") as zip_ref1:\n",
    "    zip_ref1.extractall(\"square_T1\")\n",
    "with zipfile.ZipFile(\"square_T2.zip\",\"r\") as zip_ref2:\n",
    "    zip_ref2.extractall(\"square_T2\")\n",
    "with zipfile.ZipFile(\"square_T3.zip\",\"r\") as zip_ref3:\n",
    "    zip_ref3.extractall(\"square_T3\")\n",
    "with zipfile.ZipFile(\"square_T4.zip\",\"r\") as zip_ref4:\n",
    "    zip_ref4.extractall(\"square_T4\")\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"triangle_T1.zip\",\"r\") as zip_ref5:\n",
    "    zip_ref5.extractall(\"triangle_T1\")\n",
    "with zipfile.ZipFile(\"triangle_T2.zip\",\"r\") as zip_ref6:\n",
    "    zip_ref6.extractall(\"triangle_T2\")\n",
    "with zipfile.ZipFile(\"triangle_T3.zip\",\"r\") as zip_ref7:\n",
    "    zip_ref7.extractall(\"triangle_T3\")\n",
    "with zipfile.ZipFile(\"triangle_T4.zip\",\"r\") as zip_ref8:\n",
    "    zip_ref8.extractall(\"triangle_T4\")\n",
    "with zipfile.ZipFile(\"square_10T.zip\",\"r\") as zip_ref9:\n",
    "    zip_ref9.extractall(\"square_10T\")\n",
    "with zipfile.ZipFile(\"triangle_10T.zip\",\"r\") as zip_ref10:\n",
    "    zip_ref10.extractall(\"triangle_10T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "square_T1 = []\n",
    "square_T2 = []\n",
    "square_T3 = []\n",
    "square_T4 = []\n",
    "\n",
    "for i in range(250):\n",
    "    square_T1.append(np.loadtxt('./square_T1/square_T1/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "    square_T2.append(np.loadtxt('./square_T2/square_T2/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "    square_T3.append(np.loadtxt('./square_T3/square_T3/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "    square_T4.append(np.loadtxt('./square_T4/square_T4/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "\n",
    "print(len(square_T1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to handle each of the 10 configurations of each of the Test Versions of the squares and triangles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    globals()['test_square' + str(k)] = []\n",
    "    globals()['test_triangle' + str(k)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_square = []\n",
    "test_triangle = []\n",
    "for k in range(10):\n",
    "    for i in range(10):\n",
    "        globals()['test_square' + str(k)].append(np.loadtxt('./square_10T/square_10T/T'+str(k).zfill(2) + \"#\" + str(i).zfill(2), delimiter=',')) ## Change to your local directory!\n",
    "        globals()['test_triangle' + str(k)].append(np.loadtxt('./triangle_10T/triangle_10T/T'+str(k).zfill(2) + \"#\" + str(i).zfill(2), delimiter=',')) ## Change to your local directory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_square2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_triangle0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: \n",
    "### Retrain your SVM (SVC) to train the classi\f",
    "cation model on both datasets to classify the 4 temperature classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two part problem - we need to do this for both the square and triangular lattices.\n",
    "First, this is demonstrated for the Square Lattices below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_img = np.concatenate((square_T1,square_T2,square_T3,square_T4))\n",
    "data_img = data_img.reshape((-1, 32*32))\n",
    "a = np.empty(len(square_T1))\n",
    "a.fill(3)\n",
    "b = np.empty(len(square_T2))\n",
    "b.fill(2)\n",
    "c = np.empty(len(square_T3))\n",
    "c.fill(1)\n",
    "d = np.empty(len(square_T4))\n",
    "d.fill(0)\n",
    "data_label = np.concatenate((a,b,c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't Set Random State to 0\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    data_img, data_label, test_size= 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "#clf = SVC(gamma=1/2048)\n",
    "clf = SVC(C=1000,kernel='rbf',gamma=1/2048,cache_size=8000,probability=False)\n",
    "clf.fit(train_img, train_lbl)\n",
    "score = clf.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19757522  0.97483273  2.01309919  3.2096433 ]\n",
      " [-0.2769852   0.96655077  2.15646598  3.15396846]\n",
      " [-0.21968128  0.94532468  2.06174205  3.21261454]\n",
      " [-0.23227257  0.94980655  2.07285374  3.20961228]\n",
      " [-0.48670659  0.91741087  2.24740417  3.32189155]]\n"
     ]
    }
   ],
   "source": [
    "dec1 = clf.decision_function(data_img)\n",
    "print(dec1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0181640625\n",
      "0.0236328125\n",
      "0.0212890625\n",
      "0.0673828125\n",
      "-0.0140625\n",
      "0.0171875\n",
      "0.016796875\n",
      "-0.05\n",
      "-0.0142578125\n",
      "-0.0703125\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    print(np.mean(globals()['test_triangle' + str(k)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_square1_np = np.array(test_square1)\n",
    "test_square1_reshaped = test_square1_np.reshape((-1, 32*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.09463131,  3.31623486,  0.97715298, -0.38801916],\n",
       "       [ 1.9709247 ,  3.25033448,  1.04171918, -0.26297837],\n",
       "       [ 2.03074118,  3.31715569,  1.00647557, -0.35437245],\n",
       "       [ 2.06032578,  3.19129267,  0.95334557, -0.20496402],\n",
       "       [ 1.98137599,  3.19573642,  1.11899252, -0.29610492]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.09463131  3.31623486  0.97715298 -0.38801916]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-3e76368e39ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "dec = (clf.decision_function(test_square1_reshaped))\n",
    "average = 0\n",
    "for k in dec:\n",
    "    print(k)\n",
    "    if(k >= 1):\n",
    "        average = average + 10\n",
    "    else:\n",
    "        average = average + 0.4\n",
    "print(average/len(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: \n",
    "### Train a fully connected neural network to do the classi\f",
    "cation, on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000,100), random_state=1)\n",
    "clf1.fit(train_img, train_lbl)\n",
    "score = clf1.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C:\n",
    "### Train a convolutional neural network to do the classi\f",
    "cation, on both datasets. Make a table of your performance numbers for these three different models and upload these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "train_img = train_img.reshape(800,32,32,1)\n",
    "test_img = test_img.reshape(200,32,32,1)\n",
    "train_lbl = np_utils.to_categorical(train_lbl)\n",
    "\n",
    "test_lbl = np_utils.to_categorical(test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/15\n",
      " - 1s - loss: 5.5674 - acc: 0.4219 - val_loss: 4.4799 - val_acc: 0.2687\n",
      "Epoch 2/15\n",
      " - 0s - loss: 6.0921 - acc: 0.3359 - val_loss: 8.4473 - val_acc: 0.2562\n",
      "Epoch 3/15\n",
      " - 0s - loss: 8.2927 - acc: 0.2547 - val_loss: 8.3447 - val_acc: 0.2625\n",
      "Epoch 4/15\n",
      " - 0s - loss: 7.8824 - acc: 0.2406 - val_loss: 8.2718 - val_acc: 0.2625\n",
      "Epoch 5/15\n",
      " - 0s - loss: 7.8362 - acc: 0.2406 - val_loss: 8.2608 - val_acc: 0.2625\n",
      "Epoch 6/15\n",
      " - 0s - loss: 7.8325 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 7/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 8/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 9/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 10/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 11/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 12/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 13/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 14/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
      "Epoch 15/15\n",
      " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, kernel_size = 3, input_shape=(32,32,1)))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(4))\n",
    "model1.compile(optimizer = sgd, loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "history = model1.fit(train_img,train_lbl, epochs=15, batch_size=32, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 527us/step\n",
      "[8.623410949707031, 0.27]\n"
     ]
    }
   ],
   "source": [
    "score = model1.evaluate(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/5\n",
      " - 7s - loss: nan - acc: 0.2422 - val_loss: nan - val_acc: 0.2500\n",
      "Epoch 2/5\n",
      " - 5s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
      "Epoch 3/5\n",
      " - 4s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
      "Epoch 4/5\n",
      " - 3s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
      "Epoch 5/5\n",
      " - 2s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "history = model.fit(train_img,train_lbl, epochs=5, batch_size=32, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D:\n",
    "### We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the termperatures of these spin configurations. Upload your results to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Transfer Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-47aa4cf70846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m feature_layers = [\n\u001b[0;32m----> 4\u001b[0;31m     Conv2D(filters, kernel_size,\n\u001b[0m\u001b[1;32m      5\u001b[0m            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            input_shape=input_shape),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filters' is not defined"
     ]
    }
   ],
   "source": [
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "\n",
    "feature_layers = [\n",
    "    Conv2D(filters, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=input_shape),\n",
    "    Activation('relu'),\n",
    "    Conv2D(filters, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "# create complete model\n",
    "model = Sequential(feature_layers + classification_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 2: Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "### Show that the square lattice data can be trained per-fectly with inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b96d508f7c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inception Algorithm:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_image_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_T1\u001b[0m  \u001b[0;31m# A batch of images with shape [batch_size, height, width, 3].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Features with shape [batch_size, num_features].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# Inception Algorithm:\n",
    "module = hub.Module(\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\")\n",
    "height, width = hub.get_expected_image_size(module)\n",
    "images = square_T1  # A batch of images with shape [batch_size, height, width, 3].\n",
    "features = module(images)  # Features with shape [batch_size, num_features].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a)\n",
    "### Train the data in \"fluid org\" with inception. Note that Inception takes pictures with 299 x 299 pixels. Shown that these images can be classified into different Ra nicely with inception. (You might find the python package PIL to be helpful here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
