{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AM_216_PSET_3_Problem_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vnkn17/AM216_Work/blob/master/AM_216_PSET_3_Problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FtUvBiTA9e1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem Set 3\n",
        "\n",
        "## By Varun Krishnan"
      ]
    },
    {
      "metadata": {
        "id": "IAZU_w649e1V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n"
      ]
    },
    {
      "metadata": {
        "id": "urqCzoFV9e1W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Problem Specification: \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDwcnJQE9e1b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Statements"
      ]
    },
    {
      "metadata": {
        "id": "QyOGBwZ49e1d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install(package):\n",
        "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "chxdqWQ59e1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "install(\"tensorflow>=1.7.0\")\n",
        "install('tensorflow-hub')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zlgyqPMQ9e1k",
        "colab_type": "code",
        "outputId": "32a562e2-fa01-4591-bfd6-643657d1f808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize as spopt\n",
        "import tensorflow_hub as hub\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0305 22:25:26.358505 140330784204672 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cM_Q61nc9e1o",
        "colab_type": "code",
        "outputId": "18a2c798-cdda-40cd-8426-6e42671757e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XA6asepQ9e1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Loading Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0jzbcWV9e1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"square_T1.zip\",\"r\") as zip_ref1:\n",
        "    zip_ref1.extractall(\"square_T1\")\n",
        "with zipfile.ZipFile(\"square_T2.zip\",\"r\") as zip_ref2:\n",
        "    zip_ref2.extractall(\"square_T2\")\n",
        "with zipfile.ZipFile(\"square_T3.zip\",\"r\") as zip_ref3:\n",
        "    zip_ref3.extractall(\"square_T3\")\n",
        "with zipfile.ZipFile(\"square_T4.zip\",\"r\") as zip_ref4:\n",
        "    zip_ref4.extractall(\"square_T4\")\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"triangle_T1.zip\",\"r\") as zip_ref5:\n",
        "    zip_ref5.extractall(\"triangle_T1\")\n",
        "with zipfile.ZipFile(\"triangle_T2.zip\",\"r\") as zip_ref6:\n",
        "    zip_ref6.extractall(\"triangle_T2\")\n",
        "with zipfile.ZipFile(\"triangle_T3.zip\",\"r\") as zip_ref7:\n",
        "    zip_ref7.extractall(\"triangle_T3\")\n",
        "with zipfile.ZipFile(\"triangle_T4.zip\",\"r\") as zip_ref8:\n",
        "    zip_ref8.extractall(\"triangle_T4\")\n",
        "with zipfile.ZipFile(\"square_10T.zip\",\"r\") as zip_ref9:\n",
        "    zip_ref9.extractall(\"square_10T\")\n",
        "with zipfile.ZipFile(\"triangle_10T.zip\",\"r\") as zip_ref10:\n",
        "    zip_ref10.extractall(\"triangle_10T\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsbfwD5D9e1y",
        "colab_type": "code",
        "outputId": "534d637b-0b1e-4275-8a82-3fa2d8a68e1c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "square_T1 = []\n",
        "square_T2 = []\n",
        "square_T3 = []\n",
        "square_T4 = []\n",
        "\n",
        "for i in range(250):\n",
        "    square_T1.append(np.loadtxt('./square_T1/square_T1/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
        "    square_T2.append(np.loadtxt('./square_T2/square_T2/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
        "    square_T3.append(np.loadtxt('./square_T3/square_T3/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
        "    square_T4.append(np.loadtxt('./square_T4/square_T4/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
        "\n",
        "print(len(square_T1[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3M6UZq4g9e15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code to handle each of the 10 configurations of each of the Test Versions of the squares and triangles, respectively."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FmxokIj9e19",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for k in range(10):\n",
        "    globals()['test_square' + str(k)] = []\n",
        "    globals()['test_triangle' + str(k)] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9X6Oo5Sl9e2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_square = []\n",
        "test_triangle = []\n",
        "for k in range(10):\n",
        "    for i in range(10):\n",
        "        globals()['test_square' + str(k)].append(np.loadtxt('./square_10T/square_10T/T'+str(k).zfill(2) + \"#\" + str(i).zfill(2), delimiter=',')) ## Change to your local directory!\n",
        "        globals()['test_triangle' + str(k)].append(np.loadtxt('./triangle_10T/triangle_10T/T'+str(k).zfill(2) + \"#\" + str(i).zfill(2), delimiter=',')) ## Change to your local directory!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gNewGyy9e2F",
        "colab_type": "code",
        "outputId": "0a7c6425-efb6-4f24-bb88-1321206e28e9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(test_square2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "H9pOELUZ9e2N",
        "colab_type": "code",
        "outputId": "23a2ff0f-0f05-48b9-9882-29e8eba69673",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(test_triangle0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "jwwnV0md9e2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part A: \n",
        "### Retrain your SVM (SVC) to train the classi\fcation model on both datasets to classify the 4 temperature classes."
      ]
    },
    {
      "metadata": {
        "id": "vcb1t-JC9e2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a two part problem - we need to do this for both the square and triangular lattices.\n",
        "First, this is demonstrated for the Square Lattices below:"
      ]
    },
    {
      "metadata": {
        "id": "KpO6nTdD9e2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_img = np.concatenate((square_T1,square_T2,square_T3,square_T4))\n",
        "data_img = data_img.reshape((-1, 32*32))\n",
        "a = np.empty(len(square_T1))\n",
        "a.fill(3)\n",
        "b = np.empty(len(square_T2))\n",
        "b.fill(2)\n",
        "c = np.empty(len(square_T3))\n",
        "c.fill(1)\n",
        "d = np.empty(len(square_T4))\n",
        "d.fill(0)\n",
        "data_label = np.concatenate((a,b,c,d))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T83_Ezrc9e2Y",
        "colab_type": "code",
        "outputId": "85378905-ca92-4a8a-94af-840b2ec664e5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(data_img[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "5qr11KrF9e2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't Set Random State to 0\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
        "    data_img, data_label, test_size= 0.2, random_state = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DrslHVaN9e2g",
        "colab_type": "code",
        "outputId": "9b247f6d-4577-4782-81b6-daf5648d8f8e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "#clf = SVC(gamma=1/2048)\n",
        "clf = SVC(C=1000,kernel='rbf',gamma=1/2048,cache_size=8000,probability=False)\n",
        "clf.fit(train_img, train_lbl)\n",
        "score = clf.score(test_img, test_lbl)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jbeBKCO09e2j",
        "colab_type": "code",
        "outputId": "52cd90c2-c85a-4c4b-852f-d64aa8b22fe4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec1 = clf.decision_function(data_img)\n",
        "print(dec1[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.19757522  0.97483273  2.01309919  3.2096433 ]\n",
            " [-0.2769852   0.96655077  2.15646598  3.15396846]\n",
            " [-0.21968128  0.94532468  2.06174205  3.21261454]\n",
            " [-0.23227257  0.94980655  2.07285374  3.20961228]\n",
            " [-0.48670659  0.91741087  2.24740417  3.32189155]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ATI8ayKV9e2o",
        "colab_type": "code",
        "outputId": "546beb44-f880-4c60-f0f4-7caafbf57d71",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for k in range(10):\n",
        "    print(np.mean(globals()['test_triangle' + str(k)]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.0181640625\n",
            "0.0236328125\n",
            "0.0212890625\n",
            "0.0673828125\n",
            "-0.0140625\n",
            "0.0171875\n",
            "0.016796875\n",
            "-0.05\n",
            "-0.0142578125\n",
            "-0.0703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KCHWFg539e2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_square1_np = np.array(test_square1)\n",
        "test_square1_reshaped = test_square1_np.reshape((-1, 32*32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UXGvbK-v9e2u",
        "colab_type": "code",
        "outputId": "62b1b1c6-89e4-400a-e673-fc915699922e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.09463131,  3.31623486,  0.97715298, -0.38801916],\n",
              "       [ 1.9709247 ,  3.25033448,  1.04171918, -0.26297837],\n",
              "       [ 2.03074118,  3.31715569,  1.00647557, -0.35437245],\n",
              "       [ 2.06032578,  3.19129267,  0.95334557, -0.20496402],\n",
              "       [ 1.98137599,  3.19573642,  1.11899252, -0.29610492]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "0Gd38bGP9e2x",
        "colab_type": "code",
        "outputId": "62f0698c-69bc-4eac-88da-2ca23d4c8af5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dec = (clf.decision_function(test_square1_reshaped))\n",
        "average = 0\n",
        "for k in dec:\n",
        "    print(k)\n",
        "    if(k >= 1):\n",
        "        average = average + 10\n",
        "    else:\n",
        "        average = average + 0.4\n",
        "print(average/len(arr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.09463131  3.31623486  0.97715298 -0.38801916]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-3e76368e39ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "f4sUnJlZ9e22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part B: \n",
        "### Train a fully connected neural network to do the classi\fcation, on both datasets."
      ]
    },
    {
      "metadata": {
        "id": "NFsALXbv9e24",
        "colab_type": "code",
        "outputId": "70a90b03-e9a5-4312-d5cf-9e8381e1db7a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000,100), random_state=1)\n",
        "clf1.fit(train_img, train_lbl)\n",
        "score = clf1.score(test_img, test_lbl)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EF2M0wBz9e3A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part C:\n",
        "### Train a convolutional neural network to do the classi\fcation, on both datasets. Make a table of your performance numbers for these three different models and upload these numbers."
      ]
    },
    {
      "metadata": {
        "id": "37tTG5zr9e3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "train_img = train_img.reshape(800,32,32,1)\n",
        "test_img = test_img.reshape(200,32,32,1)\n",
        "train_lbl = np_utils.to_categorical(train_lbl)\n",
        "\n",
        "test_lbl = np_utils.to_categorical(test_lbl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYojF7sX9e3D",
        "colab_type": "code",
        "outputId": "3782db65-3089-4060-d189-6683018900ec",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, kernel_size = 3, input_shape=(32,32,1)))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(4))\n",
        "model1.compile(optimizer = sgd, loss='categorical_crossentropy',metrics = ['accuracy'])\n",
        "\n",
        "history = model1.fit(train_img,train_lbl, epochs=15, batch_size=32, verbose=2, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 640 samples, validate on 160 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 5.5674 - acc: 0.4219 - val_loss: 4.4799 - val_acc: 0.2687\n",
            "Epoch 2/15\n",
            " - 0s - loss: 6.0921 - acc: 0.3359 - val_loss: 8.4473 - val_acc: 0.2562\n",
            "Epoch 3/15\n",
            " - 0s - loss: 8.2927 - acc: 0.2547 - val_loss: 8.3447 - val_acc: 0.2625\n",
            "Epoch 4/15\n",
            " - 0s - loss: 7.8824 - acc: 0.2406 - val_loss: 8.2718 - val_acc: 0.2625\n",
            "Epoch 5/15\n",
            " - 0s - loss: 7.8362 - acc: 0.2406 - val_loss: 8.2608 - val_acc: 0.2625\n",
            "Epoch 6/15\n",
            " - 0s - loss: 7.8325 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 7/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 8/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 9/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 10/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 11/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 12/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 13/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 14/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n",
            "Epoch 15/15\n",
            " - 0s - loss: 7.8324 - acc: 0.2406 - val_loss: 8.2605 - val_acc: 0.2625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5hz2mCO9e3G",
        "colab_type": "code",
        "outputId": "4fecb227-6b5b-4146-eb65-adcedb5e91b9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model1.evaluate(test_img, test_lbl)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 527us/step\n",
            "[8.623410949707031, 0.27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FGKxLaoo9e3M",
        "colab_type": "code",
        "outputId": "c1e61440-406b-4e38-c32e-1206cc925172",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(data_img[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "JiT428WN9e3P",
        "colab_type": "code",
        "outputId": "3e7e1987-760f-4afd-82d6-4c09b6916ef4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation='relu'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
        "\n",
        "history = model.fit(train_img,train_lbl, epochs=5, batch_size=32, verbose=2, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 640 samples, validate on 160 samples\n",
            "Epoch 1/5\n",
            " - 7s - loss: nan - acc: 0.2422 - val_loss: nan - val_acc: 0.2500\n",
            "Epoch 2/5\n",
            " - 5s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
            "Epoch 3/5\n",
            " - 4s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
            "Epoch 4/5\n",
            " - 3s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n",
            "Epoch 5/5\n",
            " - 2s - loss: nan - acc: 0.2453 - val_loss: nan - val_acc: 0.2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fAQmXo7h9e3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BiEaKWuo9e3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part D:\n",
        "### We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the termperatures of these spin configurations. Upload your results to Kaggle."
      ]
    },
    {
      "metadata": {
        "id": "LrEW9zDj9e3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBsXT93d9e3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part E: Transfer Learning\n"
      ]
    },
    {
      "metadata": {
        "id": "jBBstoqo9e3f",
        "colab_type": "code",
        "outputId": "904964fe-5505-4d40-c8cc-5ebdf1c7370c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define two groups of layers: feature (convolutions) and classification (dense)\n",
        "\n",
        "feature_layers = [\n",
        "    Conv2D(filters, kernel_size,\n",
        "           padding='valid',\n",
        "           input_shape=input_shape),\n",
        "    Activation('relu'),\n",
        "    Conv2D(filters, kernel_size),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=pool_size),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "]\n",
        "\n",
        "classification_layers = [\n",
        "    Dense(128),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes),\n",
        "    Activation('softmax')\n",
        "]\n",
        "\n",
        "# create complete model\n",
        "model = Sequential(feature_layers + classification_layers)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filters' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-47aa4cf70846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m feature_layers = [\n\u001b[0;32m----> 4\u001b[0;31m     Conv2D(filters, kernel_size,\n\u001b[0m\u001b[1;32m      5\u001b[0m            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            input_shape=input_shape),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filters' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "RieG38KU9e3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Inception"
      ]
    },
    {
      "metadata": {
        "id": "1kw8v48W9e3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: \n",
        "### Show that the square lattice data can be trained per-fectly with inception."
      ]
    },
    {
      "metadata": {
        "id": "bpf0-uW_9e3o",
        "colab_type": "code",
        "outputId": "4e6230b5-30f0-4664-b38f-4740ac18747d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inception Algorithm:\n",
        "module = hub.Module(\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\")\n",
        "height, width = hub.get_expected_image_size(module)\n",
        "images = square_T1  # A batch of images with shape [batch_size, height, width, 3].\n",
        "features = module(images)  # Features with shape [batch_size, num_features].\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hub' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b96d508f7c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inception Algorithm:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_image_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_T1\u001b[0m  \u001b[0;31m# A batch of images with shape [batch_size, height, width, 3].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Features with shape [batch_size, num_features].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "RLlw6tpZ9e3q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2:\n"
      ]
    },
    {
      "metadata": {
        "id": "t45Q0lCb9e3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJqYnxPt9e3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2a)\n",
        "### Train the data in \"fluid org\" with inception. Note that Inception takes pictures with 299 x 299 pixels. Shown that these images can be classified into different Ra nicely with inception. (You might find the python package PIL to be helpful here.)"
      ]
    },
    {
      "metadata": {
        "id": "D3jcNsTj9e3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BKqynWW9e3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## 2b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YGa7nuF9e3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}